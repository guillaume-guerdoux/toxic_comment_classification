{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, GRU, Embedding, Dropout, Activation, Add\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Â Load Data\n",
    "train_data_path = \"data/train.csv\"\n",
    "\n",
    "train_set = pd.read_csv(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               False\n",
       "comment_text     False\n",
       "toxic            False\n",
       "severe_toxic     False\n",
       "obscene          False\n",
       "threat           False\n",
       "insult           False\n",
       "identity_hate    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "Y = train_set[train_set.columns[2:]]\n",
    "list_sentences_train = train_set[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571, 6), (159571,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape, list_sentences_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach that we are taking is to feed the comments into the LSTM as part of the neural network but we can't just feed the words as it is.\n",
    "\n",
    "So this is what we are going to do:\n",
    "\n",
    "1. Tokenization - We need to break down the sentence into unique words. For eg, \"I love cats and love dogs\" will become [\"I\",\"love\",\"cats\",\"and\",\"dogs\"]\n",
    "2. Indexing - We put the words in a dictionary-like structure and give them an index each For eg, {1:\"I\",2:\"love\",3:\"cats\",4:\"and\",5:\"dogs\"}\n",
    "3. Index Representation- We could represent the sequence of words in the comments in the form of index, and feed this chain of index into our LSTM. For eg, [1,2,3,4,2,5]\n",
    "\n",
    "Fortunately, Keras has made our lives so much easier. If you are using the vanilla Tensorflow, you probably need to implement your own dictionary structure and handle the indexing yourself. In Keras, all the above steps can be done in 4 lines of code. Note that we have to define the number of unique words in our dictionary when tokenizing the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's treat the text and get extra features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment_list = list(list_sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \n",
    "    def __init__(self, punctuation_set):\n",
    "        self.punctuation_set = punctuation_set\n",
    "        \n",
    "    def extract_features(self, comments_list):\n",
    "        \"\"\" This function aims at extracting features from \n",
    "        list of comments\n",
    "        1. remove \\n\n",
    "        2. Count number of punctuation then divide it by letter number in doc\n",
    "        3. Count number of capital letter then divide it by letter number in doc\n",
    "        4. Count unique_words_nb / nb_words\n",
    "        5. Count \n",
    "        \"\"\"\n",
    "        sentences_count = []\n",
    "        punctuation_list = []\n",
    "        capital_letters_list = []\n",
    "        unique_words_list = []\n",
    "        for comment in comments_list:\n",
    "            sentences_count.append(len(re.findall(\"\\n\",str(comment)))+1)\n",
    "            clean_comment = comment.replace('\\n', ' ')\n",
    "            nb_letters = len(clean_comment.replace(' ', ''))\n",
    "            punctuation_list.append(sum(map(clean_comment.count, self.punctuation_set))/nb_letters)\n",
    "            capital_letters_list.append(len(re.findall(r'[A-Z]', clean_comment))/nb_letters)\n",
    "            unique_words_list.append(len(set(clean_comment.split()))/len(clean_comment.split()))\n",
    "        return sentences_count, punctuation_list, capital_letters_list, unique_words_list\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor(set(string.punctuation))\n",
    "sentences_count, punctuation_list, capital_letters_list, unique_words_list = feature_extractor.extract_features(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_list_sentences_train = list_sentences_train.to_frame()\n",
    "df_list_sentences_train['nb_sentences'] = sentences_count\n",
    "df_list_sentences_train['nb_punctuation'] = punctuation_list\n",
    "df_list_sentences_train['nb_capital'] = capital_letters_list\n",
    "df_list_sentences_train['nb_unique_words'] = unique_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>nb_sentences</th>\n",
       "      <th>nb_punctuation</th>\n",
       "      <th>nb_capital</th>\n",
       "      <th>nb_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.045045</td>\n",
       "      <td>0.076577</td>\n",
       "      <td>0.953488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.041420</td>\n",
       "      <td>0.021696</td>\n",
       "      <td>0.725664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.048718</td>\n",
       "      <td>0.017949</td>\n",
       "      <td>0.843373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  nb_sentences  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...             2   \n",
       "1  D'aww! He matches this background colour I'm s...             1   \n",
       "2  Hey man, I'm really not trying to edit war. It...             1   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...             5   \n",
       "4  You, sir, are my hero. Any chance you remember...             1   \n",
       "5  \"\\n\\nCongratulations from me as well, use the ...             3   \n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK             1   \n",
       "7  Your vandalism to the Matt Shirvington article...             1   \n",
       "8  Sorry if the word 'nonsense' was offensive to ...             1   \n",
       "9  alignment on this subject and which are contra...             1   \n",
       "\n",
       "   nb_punctuation  nb_capital  nb_unique_words  \n",
       "0        0.045045    0.076577         0.953488  \n",
       "1        0.126316    0.084211         1.000000  \n",
       "2        0.031250    0.020833         0.928571  \n",
       "3        0.041420    0.021696         0.725664  \n",
       "4        0.090909    0.036364         1.000000  \n",
       "5        0.076923    0.019231         0.923077  \n",
       "6        0.000000    1.000000         1.000000  \n",
       "7        0.042105    0.042105         1.000000  \n",
       "8        0.048718    0.017949         0.843373  \n",
       "9        0.000000    0.033898         1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list_sentences_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_sentences</th>\n",
       "      <th>nb_punctuation</th>\n",
       "      <th>nb_capital</th>\n",
       "      <th>nb_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.045045</td>\n",
       "      <td>0.076577</td>\n",
       "      <td>0.953488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.041420</td>\n",
       "      <td>0.021696</td>\n",
       "      <td>0.725664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nb_sentences  nb_punctuation  nb_capital  nb_unique_words\n",
       "0             2        0.045045    0.076577         0.953488\n",
       "1             1        0.126316    0.084211         1.000000\n",
       "2             1        0.031250    0.020833         0.928571\n",
       "3             5        0.041420    0.021696         0.725664\n",
       "4             1        0.090909    0.036364         1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_features = df_list_sentences_train[df_list_sentences_train.columns[1:]]\n",
    "fixed_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210337"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there's still 1 problem! What if some comments are terribly long, while some are just 1 word? Wouldn't our indexed-sentence look like this:\n",
    "\n",
    "Comment #1: [8,9,3,7,3,6,3,6,3,6,2,3,4,9]\n",
    "\n",
    "Comment #2: [1,2]\n",
    "\n",
    "And we have to feed a stream of data that has a consistent length(fixed number of features) isn't it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8d06a015c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_words_in_docs = [len(doc) for doc in list_tokenized_train]\n",
    "plt.hist(total_words_in_docs,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 300\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571, 300), (159571, 4))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t.shape, fixed_features.as_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_final = np.concatenate((X_t, fixed_features.as_matrix()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((143613, 304), (15958, 304), 143613, 15958)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_final, Y, test_size=0.1)\n",
    "X_train.shape, X_val.shape, len(Y_train), len(Y_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[15:79, maxlen:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.index_train = 0\n",
    "        self.index_val = 0\n",
    "    \n",
    "    def generate_data(self, batch_size, train):\n",
    "        while True:\n",
    "            comments = []\n",
    "            fixed_features = []\n",
    "            labels = []\n",
    "            if train:\n",
    "                if self.index_train + batch_size < len(X_train): \n",
    "                    comments.append(X_train[self.index_train:self.index_train + batch_size, :maxlen])\n",
    "                    fixed_features.append(X_train[self.index_train:self.index_train + batch_size, maxlen:])\n",
    "                    labels.append(Y_train[self.index_train:self.index_train + batch_size])\n",
    "                    self.index_train += batch_size\n",
    "                else:\n",
    "                    rest_train = (self.index_train + batch_size) % len(X_train)\n",
    "                    comments = [np.append(X_train[self.index_train:, :maxlen], X_train[:rest_train, :maxlen], axis=0)]\n",
    "                    fixed_features = [np.append(X_train[self.index_train:, maxlen:], X_train[:rest_train, maxlen:], axis=0)]\n",
    "                    labels = [np.append(Y_train[self.index_train:], Y_train[:rest_train], axis=0)]\n",
    "                    self.index_train = rest_train\n",
    "            else:\n",
    "                if self.index_val + batch_size < len(X_val): \n",
    "                    comments.append(X_val[self.index_val:self.index_val + batch_size, :maxlen])\n",
    "                    fixed_features.append(X_val[self.index_val:self.index_val + batch_size, maxlen:])\n",
    "                    labels.append(Y_val[self.index_val:self.index_val + batch_size])\n",
    "                    self.index_val += batch_size\n",
    "                else:\n",
    "                    rest_val = (self.index_val + batch_size) % len(X_val)\n",
    "                    comments = [np.append(X_val[self.index_val:, :maxlen], X_val[:rest_val, :maxlen], axis=0)]\n",
    "                    fixed_features = [np.append(X_val[self.index_val:, maxlen:], X_val[:rest_val, maxlen:], axis=0)]\n",
    "                    labels = [np.append(Y_val[self.index_val:], Y_val[:rest_val], axis=0)]\n",
    "                    self.index_val = rest_val\n",
    "            yield [np.array(comments).reshape(batch_size, maxlen), np.array(fixed_features).reshape(batch_size, 4)], labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "comment_layer_input = Input(shape=(maxlen,))\n",
    "comment_layer_embed = Embedding(input_dim=max_features, output_dim=embed_size)(comment_layer_input)\n",
    "comment_layer_gru = GRU(32, return_sequences=True, name='gru_layer')(comment_layer_embed)\n",
    "comment_layer = GlobalMaxPool1D()(comment_layer_gru)\n",
    "comment_layer = Dropout(0.2)(comment_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fixed_features_input = Input(shape=(4,))\n",
    "fixed_features_dense_layer = Dense(units=32, activation='relu')(fixed_features_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_layer = Add()([comment_layer, fixed_features_dense_layer])\n",
    "global_dense_layer = Dense(64, activation=\"relu\")(merge_layer)\n",
    "global_dense_layer = Dropout(0.2)(global_dense_layer)\n",
    "output = Dense(len(list_classes), activation=\"sigmoid\")(global_dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 300, 128)     2560000     input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gru_layer (GRU)                 (None, 300, 32)      15456       embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 32)           0           gru_layer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 32)           0           global_max_pooling1d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 32)           160         input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 32)           0           dropout_16[0][0]                 \n",
      "                                                                 dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 64)           2112        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 64)           0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 6)            390         dropout_17[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,578,118\n",
      "Trainable params: 2,578,118\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[comment_layer_input, fixed_features_input], outputs=output)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=5)\n",
    "callbacks_list = [earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaume/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  import sys\n",
      "/home/guillaume/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., callbacks=[<keras.ca..., validation_steps=150, validation_data=<generator..., epochs=50, steps_per_epoch=200)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 81/200 [===========>..................] - ETA: 38s - loss: 0.3059 - acc: 0.9089"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-ec2c155eeba4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                         validation_data=data_generator.generate_data(\n\u001b[1;32m      6\u001b[0m                             batch_size, False),\n\u001b[0;32m----> 7\u001b[0;31m                         validation_steps=150, callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "model_info = model.fit_generator(data_generator.generate_data(batch_size, True),\n",
    "                        samples_per_epoch=200, nb_epoch=epochs,\n",
    "                        validation_data=data_generator.generate_data(\n",
    "                            batch_size, False),\n",
    "                        validation_steps=150, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(maxlen,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "embed_layer = Embedding(max_features, embed_size)(input_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the short line of code that defines the LSTM layer, it's easy to miss the required input dimensions. LSTM takes in a tensor of [Batch Size, Time Steps, Number of Inputs]. Batch size is the number of samples in a batch, time steps is the number of recursion it runs for each input, or it could be pictured as the number of \"A\"s in the above picture. Lastly, number of inputs is the number of variables(number of words in each sentence in our case) you pass into LSTM as pictured in \"x\" above.\n",
    "\n",
    "We can make use of the output from the previous embedding layer which outputs a 3-D tensor of (None, 200, 128) into the LSTM layer. What it does is going through the samples, recursively run the LSTM model for 200 times, passing in the coordinates of the words each time. And because we want the unrolled version, we will receive a Tensor shape of (None, 200, 60), where 60 is the output dimension we have defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = LSTM(32, return_sequences=True, name='lstm_layer')(embed_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = GlobalMaxPool1D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Dropout(0.2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Dense(len(list_classes), activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=input_layer, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 300, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 300, 32)           20608     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 2,583,110\n",
      "Trainable params: 2,583,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_generator = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaume/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  import sys\n",
      "/home/guillaume/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., callbacks=[<keras.ca..., validation_steps=150, validation_data=<generator..., steps_per_epoch=200, epochs=20)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "200/200 [==============================] - 91s 454ms/step - loss: 0.2111 - acc: 0.9516 - val_loss: 0.1490 - val_acc: 0.9612\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.1540 - acc: 0.9604 - val_loss: 0.1289 - val_acc: 0.9656\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - 84s 421ms/step - loss: 0.1330 - acc: 0.9639 - val_loss: 0.1137 - val_acc: 0.9629\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 90s 448ms/step - loss: 0.0974 - acc: 0.9677 - val_loss: 0.0710 - val_acc: 0.9766\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - 72s 362ms/step - loss: 0.0666 - acc: 0.9771 - val_loss: 0.0553 - val_acc: 0.9816\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 80s 399ms/step - loss: 0.0574 - acc: 0.9798 - val_loss: 0.0524 - val_acc: 0.9819\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 90s 450ms/step - loss: 0.0615 - acc: 0.9802 - val_loss: 0.0624 - val_acc: 0.9772\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 90s 451ms/step - loss: 0.0562 - acc: 0.9813 - val_loss: 0.0572 - val_acc: 0.9808\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 91s 453ms/step - loss: 0.0576 - acc: 0.9797 - val_loss: 0.0495 - val_acc: 0.9820\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 90s 449ms/step - loss: 0.0588 - acc: 0.9793 - val_loss: 0.0549 - val_acc: 0.9805\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - 88s 439ms/step - loss: 0.0569 - acc: 0.9799 - val_loss: 0.0556 - val_acc: 0.9806\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "model_info = model.fit_generator(data_generator.generate_data(batch_size,True),\n",
    "                        samples_per_epoch=200, nb_epoch=epochs,\n",
    "                        validation_data=data_generator.generate_data(\n",
    "                            batch_size, False),\n",
    "                        validation_steps=150, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15958/15958 [==============================] - 53s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.052242176868941254, 0.98128419165803105]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
