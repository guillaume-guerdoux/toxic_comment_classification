{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Â Load Data\n",
    "train_data_path = \"data/train.csv\"\n",
    "\n",
    "train_set = pd.read_csv(train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               False\n",
       "comment_text     False\n",
       "toxic            False\n",
       "severe_toxic     False\n",
       "obscene          False\n",
       "threat           False\n",
       "insult           False\n",
       "identity_hate    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "Y = train_set[train_set.columns[2:]]\n",
    "list_sentences_train = train_set[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571, 6), (159571,))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape, list_sentences_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach that we are taking is to feed the comments into the LSTM as part of the neural network but we can't just feed the words as it is.\n",
    "\n",
    "So this is what we are going to do:\n",
    "\n",
    "1. Tokenization - We need to break down the sentence into unique words. For eg, \"I love cats and love dogs\" will become [\"I\",\"love\",\"cats\",\"and\",\"dogs\"]\n",
    "2. Indexing - We put the words in a dictionary-like structure and give them an index each For eg, {1:\"I\",2:\"love\",3:\"cats\",4:\"and\",5:\"dogs\"}\n",
    "3. Index Representation- We could represent the sequence of words in the comments in the form of index, and feed this chain of index into our LSTM. For eg, [1,2,3,4,2,5]\n",
    "\n",
    "Fortunately, Keras has made our lives so much easier. If you are using the vanilla Tensorflow, you probably need to implement your own dictionary structure and handle the indexing yourself. In Keras, all the above steps can be done in 4 lines of code. Note that we have to define the number of unique words in our dictionary when tokenizing the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's treat the text and get extra features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_list = list(list_sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \n",
    "    def __init__(self, punctuation_set):\n",
    "        self.punctuation_set = punctuation_set\n",
    "        \n",
    "    def extract_features(self, comments_list):\n",
    "        \"\"\" This function aims at extracting features from \n",
    "        list of comments\n",
    "        1. remove \\n\n",
    "        2. Count number of punctuation then divide it by letter number in doc\n",
    "        3. Count number of capital letter then divide it by letter number in doc\n",
    "        4. Count unique_words_nb / nb_words\n",
    "        5. Count \n",
    "        \"\"\"\n",
    "        sentences_count = []\n",
    "        punctuation_list = []\n",
    "        capital_letters_list = []\n",
    "        unique_words_list = []\n",
    "        for comment in comments_list:\n",
    "            sentences_count.append(len(re.findall(\"\\n\",str(comment)))+1)\n",
    "            clean_comment = comment.replace('\\n', ' ')\n",
    "            nb_letters = len(clean_comment.replace(' ', ''))\n",
    "            punctuation_list.append(sum(map(clean_comment.count, self.punctuation_set))/nb_letters)\n",
    "            capital_letters_list.append(len(re.findall(r'[A-Z]', clean_comment))/nb_letters)\n",
    "            unique_words_list.append(len(set(clean_comment.split()))/len(clean_comment.split()))\n",
    "        return sentences_count, punctuation_list, capital_letters_list, unique_words_list\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor(set(string.punctuation))\n",
    "sentences_count, punctuation_list, capital_letters_list, unique_words_list = feature_extractor.extract_features(comment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_sentences_train = list_sentences_train.to_frame()\n",
    "df_list_sentences_train['nb_sentences'] = sentences_count\n",
    "df_list_sentences_train['nb_punctuation'] = punctuation_list\n",
    "df_list_sentences_train['nb_capital'] = capital_letters_list\n",
    "df_list_sentences_train['nb_unique_words'] = unique_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>nb_sentences</th>\n",
       "      <th>nb_punctuation</th>\n",
       "      <th>nb_capital</th>\n",
       "      <th>nb_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.045045</td>\n",
       "      <td>0.076577</td>\n",
       "      <td>0.953488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.041420</td>\n",
       "      <td>0.021696</td>\n",
       "      <td>0.725664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.048718</td>\n",
       "      <td>0.017949</td>\n",
       "      <td>0.843373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  nb_sentences  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...             2   \n",
       "1  D'aww! He matches this background colour I'm s...             1   \n",
       "2  Hey man, I'm really not trying to edit war. It...             1   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...             5   \n",
       "4  You, sir, are my hero. Any chance you remember...             1   \n",
       "5  \"\\n\\nCongratulations from me as well, use the ...             3   \n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK             1   \n",
       "7  Your vandalism to the Matt Shirvington article...             1   \n",
       "8  Sorry if the word 'nonsense' was offensive to ...             1   \n",
       "9  alignment on this subject and which are contra...             1   \n",
       "\n",
       "   nb_punctuation  nb_capital  nb_unique_words  \n",
       "0        0.045045    0.076577         0.953488  \n",
       "1        0.126316    0.084211         1.000000  \n",
       "2        0.031250    0.020833         0.928571  \n",
       "3        0.041420    0.021696         0.725664  \n",
       "4        0.090909    0.036364         1.000000  \n",
       "5        0.076923    0.019231         0.923077  \n",
       "6        0.000000    1.000000         1.000000  \n",
       "7        0.042105    0.042105         1.000000  \n",
       "8        0.048718    0.017949         0.843373  \n",
       "9        0.000000    0.033898         1.000000  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list_sentences_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_sentences</th>\n",
       "      <th>nb_punctuation</th>\n",
       "      <th>nb_capital</th>\n",
       "      <th>nb_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.045045</td>\n",
       "      <td>0.076577</td>\n",
       "      <td>0.953488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.126316</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.041420</td>\n",
       "      <td>0.021696</td>\n",
       "      <td>0.725664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nb_sentences  nb_punctuation  nb_capital  nb_unique_words\n",
       "0             2        0.045045    0.076577         0.953488\n",
       "1             1        0.126316    0.084211         1.000000\n",
       "2             1        0.031250    0.020833         0.928571\n",
       "3             5        0.041420    0.021696         0.725664\n",
       "4             1        0.090909    0.036364         1.000000"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_features = df_list_sentences_train[df_list_sentences_train.columns[1:]]\n",
    "fixed_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210337"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there's still 1 problem! What if some comments are terribly long, while some are just 1 word? Wouldn't our indexed-sentence look like this:\n",
    "\n",
    "Comment #1: [8,9,3,7,3,6,3,6,3,6,2,3,4,9]\n",
    "\n",
    "Comment #2: [1,2]\n",
    "\n",
    "And we have to feed a stream of data that has a consistent length(fixed number of features) isn't it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2eda186898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_words_in_docs = [len(doc) for doc in list_tokenized_train]\n",
    "plt.hist(total_words_in_docs,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 300\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571, 300), (159571, 4))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t.shape, fixed_features.as_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   6.88000000e+02,   7.50000000e+01,\n",
       "         1.00000000e+00,   1.26000000e+02,   1.30000000e+02,\n",
       "         1.77000000e+02,   2.90000000e+01,   6.72000000e+02,\n",
       "         4.51100000e+03,   1.20520000e+04,   1.11600000e+03,\n",
       "         8.60000000e+01,   3.31000000e+02,   5.10000000e+01,\n",
       "         2.27800000e+03,   1.14480000e+04,   5.00000000e+01,\n",
       "         6.86400000e+03,   1.50000000e+01,   6.00000000e+01,\n",
       "         2.75600000e+03,   1.48000000e+02,   7.00000000e+00,\n",
       "         2.93700000e+03,   3.40000000e+01,   1.17000000e+02,\n",
       "         1.22100000e+03,   1.51900000e+04,   2.82500000e+03,\n",
       "         4.00000000e+00,   4.50000000e+01,   5.90000000e+01,\n",
       "         2.44000000e+02,   1.00000000e+00,   3.65000000e+02,\n",
       "         3.10000000e+01,   1.00000000e+00,   3.80000000e+01,\n",
       "         2.70000000e+01,   1.43000000e+02,   7.30000000e+01,\n",
       "         3.46200000e+03,   8.90000000e+01,   3.08500000e+03,\n",
       "         4.58300000e+03,   2.27300000e+03,   9.85000000e+02,\n",
       "         2.00000000e+00,   4.50450450e-02,   7.65765766e-02,\n",
       "         9.53488372e-01])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newX_fi_matrix = np.concatenate((X_t, fixed_features.as_matrix()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((143613, 300), (15958, 300), 143613, 15958)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_t, Y, test_size=0.1)\n",
    "X_train.shape, X_val.shape, len(Y_train), len(Y_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.index_train = 0\n",
    "        self.index_val = 0\n",
    "    \n",
    "    def generate_data(self, batch_size, train):\n",
    "        while True:\n",
    "            comments = []\n",
    "            labels = []\n",
    "            if train:\n",
    "                if self.index_train + batch_size < len(X_train): \n",
    "                    comments.append(X_train[self.index_train:self.index_train + batch_size])\n",
    "                    labels.append(Y_train[self.index_train:self.index_train + batch_size])\n",
    "                    self.index_train += batch_size\n",
    "                else:\n",
    "                    rest_train = (self.index_train + batch_size) % len(X_train)\n",
    "                    comments = [np.append(X_train[self.index_train:], X_train[:rest_train], axis=0)]\n",
    "                    labels = [np.append(Y_train[self.index_train:], Y_train[:rest_train], axis=0)]\n",
    "                    self.index_train = rest_train\n",
    "            else:\n",
    "                if self.index_val + batch_size < len(X_val): \n",
    "                    comments.append(X_val[self.index_val:self.index_val + batch_size])\n",
    "                    labels.append(Y_val[self.index_val:self.index_val + batch_size])\n",
    "                    self.index_val += batch_size\n",
    "                else:\n",
    "                    rest_val = (self.index_val + batch_size) % len(X_val)\n",
    "                    \n",
    "                    comments = [np.append(X_val[self.index_val:], X_val[:rest_val], axis=0)]\n",
    "                    labels = [np.append(Y_val[self.index_val:], Y_val[:rest_val], axis=0)]\n",
    "                    self.index_val = rest_val\n",
    "            yield comments, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(maxlen,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "embed_layer = Embedding(max_features, embed_size)(input_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the short line of code that defines the LSTM layer, it's easy to miss the required input dimensions. LSTM takes in a tensor of [Batch Size, Time Steps, Number of Inputs]. Batch size is the number of samples in a batch, time steps is the number of recursion it runs for each input, or it could be pictured as the number of \"A\"s in the above picture. Lastly, number of inputs is the number of variables(number of words in each sentence in our case) you pass into LSTM as pictured in \"x\" above.\n",
    "\n",
    "We can make use of the output from the previous embedding layer which outputs a 3-D tensor of (None, 200, 128) into the LSTM layer. What it does is going through the samples, recursively run the LSTM model for 200 times, passing in the coordinates of the words each time. And because we want the unrolled version, we will receive a Tensor shape of (None, 200, 60), where 60 is the output dimension we have defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = LSTM(32, return_sequences=True, name='lstm_layer')(embed_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = GlobalMaxPool1D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Dropout(0.2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Dense(len(list_classes), activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=5)\n",
    "callbacks_list = [earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=input_layer, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 300, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 300, 32)           20608     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 2,583,110\n",
      "Trainable params: 2,583,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_generator = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaume/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  import sys\n",
      "/home/guillaume/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., callbacks=[<keras.ca..., validation_steps=150, validation_data=<generator..., steps_per_epoch=200, epochs=20)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "200/200 [==============================] - 91s 454ms/step - loss: 0.2111 - acc: 0.9516 - val_loss: 0.1490 - val_acc: 0.9612\n",
      "Epoch 2/20\n",
      "200/200 [==============================] - 84s 422ms/step - loss: 0.1540 - acc: 0.9604 - val_loss: 0.1289 - val_acc: 0.9656\n",
      "Epoch 3/20\n",
      "200/200 [==============================] - 84s 421ms/step - loss: 0.1330 - acc: 0.9639 - val_loss: 0.1137 - val_acc: 0.9629\n",
      "Epoch 4/20\n",
      "200/200 [==============================] - 90s 448ms/step - loss: 0.0974 - acc: 0.9677 - val_loss: 0.0710 - val_acc: 0.9766\n",
      "Epoch 5/20\n",
      "200/200 [==============================] - 72s 362ms/step - loss: 0.0666 - acc: 0.9771 - val_loss: 0.0553 - val_acc: 0.9816\n",
      "Epoch 6/20\n",
      "200/200 [==============================] - 80s 399ms/step - loss: 0.0574 - acc: 0.9798 - val_loss: 0.0524 - val_acc: 0.9819\n",
      "Epoch 7/20\n",
      "200/200 [==============================] - 90s 450ms/step - loss: 0.0615 - acc: 0.9802 - val_loss: 0.0624 - val_acc: 0.9772\n",
      "Epoch 8/20\n",
      "200/200 [==============================] - 90s 451ms/step - loss: 0.0562 - acc: 0.9813 - val_loss: 0.0572 - val_acc: 0.9808\n",
      "Epoch 9/20\n",
      "200/200 [==============================] - 91s 453ms/step - loss: 0.0576 - acc: 0.9797 - val_loss: 0.0495 - val_acc: 0.9820\n",
      "Epoch 10/20\n",
      "200/200 [==============================] - 90s 449ms/step - loss: 0.0588 - acc: 0.9793 - val_loss: 0.0549 - val_acc: 0.9805\n",
      "Epoch 11/20\n",
      "200/200 [==============================] - 88s 439ms/step - loss: 0.0569 - acc: 0.9799 - val_loss: 0.0556 - val_acc: 0.9806\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "model_info = model.fit_generator(data_generator.generate_data(batch_size,True),\n",
    "                        samples_per_epoch=200, nb_epoch=epochs,\n",
    "                        validation_data=data_generator.generate_data(\n",
    "                            batch_size, False),\n",
    "                        validation_steps=150, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15958/15958 [==============================] - 53s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.052242176868941254, 0.98128419165803105]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
